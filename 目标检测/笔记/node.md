# 2 基于深度学习的一阶段目标检测方法
## 2.1 YOLO
### 2.1.1 算法引言
1. 速度快。避免了两阶段目标检测框架的复杂流程，直接使用快速的 回归方法处理目标检测问题
2. 背景误检率低。**基于预选框生成**的方法，其感受野只包括预选框内部的局部信息。不同于**基于滑动窗口和预选框生成**的方法，YOLO 在训练和测试过程中通过**对上下文信息编码增加其感受野**，降低了将包含物体的预选框错分为背景的概率；
3. 更强泛化能力以及具有更好的通用性。

### 2.1.2 基础原理
YOLO 将目标检测归纳为一个回归问题，旨在直接在输入图像的每个像素点上得到 物体的预选框坐标及其类别，从而获得最终的检测结果。
1. 调整图片尺寸为 $448 \times 448$
2. 输入网络使用CNN提取特征
3. 非极大抑制

YOLO端到端的训练总共分为两大网络：
1. 特征提取网络
   - 卷积层组成的神经网络 ——> 全连接层预测输出类别和坐标
   - 受Google Net启发，该网络架构由23个卷积层和2个全连接层组成
   - 使用$1 \times 1和3\times 3$代替Inception结构

2. 预测网络
   - 检测和分类同时进行
   - 将输入分为$S \times S$的网格，每个网格单元预测$B$个预选框以及其对应的分类置信度
   - **置信度：**$Pr(Object)*IOU^{truth}_{pred]}$。该置信度反应了其所对应的网格是否包含物体的概率以及该预选框与真实边界框之间的准确度。如果不包含真实边界框，则$Pr(Object)=0$。若包含，则置信度定义为预选框和真实边界框的交叠比。
   - 每个预选框包含五个变量：$x,y,w,h,IOU^{truth}_{pred}$。
   - 每个网格单元需要预测是否包含物体$Pr(Class_i|Object)$，并该预测与其对应的预选框的**个数B无关**
   - 每个预选框包含类别$Class_i$的得分为：<br>
    $Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_i)*IOU^{truth}_{pred}$
   
        即，预选框为类别$Class_i$的概率以及预选框和物体真实框的拟合程度。
    - 例子：杜宇PASCAL VOC数据集，一共包含20个种类。设$S=7,B=2,则最后预测维度为 S\times S\times (B*5+C)=7*7*30$
    - **以上步骤存在的一个缺陷**：每个格子可以预测$B$个预选框，但是最终只选择交叠比最高的预选框作为目标 检测的输出。当物体占画面比例较小时，每个格子可能会包含多个物体，但最终只能检测出其中一个。

使用均方和误差作为损失函数：$Loss=\sum^{S^2}_{i=0} coord\_Error + confidence\_Error + class_Error$。网络模型参数优化：
1. 位置相关损失（坐标损失、置信度损失）与分类误差对最终网络贡献是不同的。引入$\lambda_{coord}=5$对$coord\_Error$进行约束
2. 不包含物体网格单元的分类置信度将近似为 0，变相放大了包含物体的网格单元的分类损失在计算网络参数梯度时的影响。为了解决这个问题，YOLO$\lambda_{nobj}=0.5$对$confidence\_Error$使用进行约束
3. 相比于大物体的坐标损失，小物体的坐标损失对检测结果影响更大，所以其对应的权重也应该越高。在网络训练过程中，通过对w和h取平方根进行修正来缓解这个问题，但这种方法并未从本质上解决问题。

以下是YOLO最终的损失函数：
