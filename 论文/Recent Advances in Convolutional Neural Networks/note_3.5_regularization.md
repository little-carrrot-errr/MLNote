<!-- TOC -->

- [# **Regularization正则化**](#regularization%e6%ad%a3%e5%88%99%e5%8c%96)
- [**$l_p-norm \quad Regularization$ / lp范数正则化**](#lp-norm-quad-regularization--lp%e8%8c%83%e6%95%b0%e6%ad%a3%e5%88%99%e5%8c%96)
- [**Dropout**](#dropout)
- [**贝叶斯网络**](#%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c)
- [**DropConnect**](#dropconnect)

<!-- /TOC -->
# **Regularization正则化**
---
## **$l_p-norm \quad Regularization$ / lp范数正则化**
通过在loss function最后加一个惩罚项。假设loss function为$L(\theta,x,y)$，那么我们的正则化损失为：

> $E(\theta,x,y) =L(\theta,x,y)+\lambda R(\theta)$
>  
> $R(\theta)$就是我们的正则项，$\lambda$就是正则化强度。

[以下参考](https://www.cnblogs.com/maybe2030/p/9231231.html)

$l_p-norm$正则化通常表达式为$R(\theta) = \sum_j||\theta_j||^p_p$。当$p\geq 1$时，正则化是凸的，这使得最优化变得简单并且“renders this function attractive”。

**l1正则化**：$||x||_1=\sum^n_i|x_i|\quad$ l1范数就是向量各元素绝对值之和，也就是称为**系数规则算子（Lasso regularization）**。稀疏化可以带来两个最直接的好处：1.特征选择；2.可解释性
> **特征选择** ： 稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。[参考](https://blog.csdn.net/zhaomengszu/article/details/81537197)

> **可解释性**:模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。$P(t) = \frac{1}{1 + e^{-t}}$这个函数的曲线如下所示：

![](\img/sigmoid.png)


> 通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。[参考](https://www.cnblogs.com/aixueshuqian/p/3936892.html)

**L2范数**就是欧几里得的公式：<font size=4px>$||x||_2 =\sqrt{\sum_i^n(x_i)^2}$</font>
L2范数有很多名称，有人把它的回归叫“岭回归”（Ridge Regression），也有人叫它“权值衰减”（Weight Decay）。以L2范数作为正则项可以得到稠密解，即每个特征对应的参数w都很小，接近于0但是不为0；此外，L2范数作为正则化项，可以防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。

**L1与L2的区别**

![](/img/l1&&l2.jpg)
如上图所示，蓝色的圆圈表示问题可能的解范围，橘色的表示正则项可能的解范围。而整个目标函数（原问题+正则项）有解当且仅当两个解范围相切。从上图可以很容易地看出，由于L2范数解范围是圆，所以相切的点有很大可能不在坐标轴上，而由于L1范数是菱形（顶点是凸出来的），其相切的点更可能在坐标轴上，而坐标轴上的点有一个特点，其只有一个坐标分量不为零，其他坐标分量为零，即是稀疏的。所以有如下结论，L1范数可以导致稀疏解，L2范数导致稠密解。
从贝叶斯先验的角度看，当训练一个模型时，仅依靠当前的训练数据集是不够的，为了实现更好的泛化能力，往往需要加入先验项，而加入正则项相当于加入了一种先验。
- L1范数相当于加入了一个Laplacean先验；
- L2范数相当于加入了一个Gaussian先验。
如下图：
![](\img/laplacean&&Gaussian.png)

**正则化和过拟合**
拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

## **Dropout**
Dropout可以防止网络变得过于依赖任何一个(或任何小的组合)神经元，并可以迫使网络即使在没有某些的信息的情况下达到准确。

- Wang等人[90]提出了一种快速Dropout方法，它可以通过高斯近似的采样或积分来进行快速Dropout训练。文学士 
- [91]提出了一种自适应Dropout方法，其中每个隐变量的Dropout概率是使用一个与深度网络共享参数的**binary belief network** (?)计算的。

## **贝叶斯网络**
s[**参考**](https://blog.csdn.net/gdp12315_gu/article/details/50002195)

- 在[92]中，他们发现在1×1卷积层之前应用标准的Dropout通常会增加训练时间，并且不能防止过度拟合。因此，他们提出了一种新的Dropout方法，称为SpatialDropout。 退出，它将Dropout值扩展到整个特征映射（feature map）。这种新的Dropout方法在训练数据小的情况下效果很好。

## **DropConnect**
DropConnect[45]进一步发展了Dropout的概念，他随机的将元素权重矩阵$W$设置为0，而不是随机地将神经元的输出设置为零。DropConnect 的输出为:
><font size=4px>$y= a((R*W)x)$</font>
>   
> $R_{ij}\sim Bernoulli(p)$

此外，在训练过程中，偏差也会被masked(?).下图显示了无下降网络、Dropout网络和DropConnect网络之间的差异：
![](img/No-Drop&DropOut&DropConnect&#32;network.png)