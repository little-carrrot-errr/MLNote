<!-- TOC -->

- [**Application of CNNs**](#application-of-cnns)
  - [**Image Classification图像分类**](#image-classification%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb)
  - [**Object Tracking(对象跟踪)**](#object-tracking%e5%af%b9%e8%b1%a1%e8%b7%9f%e8%b8%aa)
  - [**Pose Estimation姿态估计**](#pose-estimation%e5%a7%bf%e6%80%81%e4%bc%b0%e8%ae%a1)
  - [**Test Detection and Recognition 文本检测和识别**](#test-detection-and-recognition-%e6%96%87%e6%9c%ac%e6%a3%80%e6%b5%8b%e5%92%8c%e8%af%86%e5%88%ab)
    - [**Text Detection 文本检测**](#text-detection-%e6%96%87%e6%9c%ac%e6%a3%80%e6%b5%8b)
    - [**Text Recognition文本识别**](#text-recognition%e6%96%87%e6%9c%ac%e8%af%86%e5%88%ab)
    - [**End-to-end Text Spotting端到端文本检测**](#end-to-end-text-spotting%e7%ab%af%e5%88%b0%e7%ab%af%e6%96%87%e6%9c%ac%e6%a3%80%e6%b5%8b)

<!-- /TOC -->

# **Application of CNNs**

## **Image Classification图像分类**
CNNs在图像分类中的应用已经很长时间了[168-171]。与其他方法相比，由于CNN在组合特征和分类学习中的能力，它可以在大规模数据集上获得更好的准确性[8,9,172]。大规模图像分类的突破出现在2012年。Krizhevsky等人[8]开发AlexNet，并在2012年ILSVRC中取得最佳成绩。在AlexNet成功之后，许多方法通过缩小过滤器大小[11]或扩大网络深度[9，10]使得CNN在分类精度上取得了显著的提高。

在多标签分类中，构建一个结构化的分类器是一个常用的图像分类的方法[173]。
- [174]的工作是在cnn中引入类别层次的最早尝试之一，其中提出了一种基于树优先（**tree-based**）的区分转移学习方法（**discriminative transfer learning**）。他们使用一种类的层次结构在相关类之间共享信息，以提高训练样例少的类别的性能。
- Wang等人[175]建立一个树结构来学习细粒度特征用于子类别识别
- Xiao等人[176]提出了一种训练方法，该方法不仅可以逐步增长网络，而且可以分层增长网络。首先使用粗分类CNN分类器将容易区分的类彼此分离，然后将那些难以分类的类路由到下游的精细类别分类器，以便进一步的预测。该体系结构遵循粗糙到精细的分类模式，可以以可承受的复杂度增加为代价，获得较低的误差。

子类别（子标签）分类是另一类发展迅速的图像识别的领域。已经有一些细粒度的图像数据集(如鸟类[178]、狗[179]、汽车[180]和植物[181])。使用对象部分信息（object part information）有利于细粒度分类[182]。一般情况下，通过定位物体的重要部分并以区分度大的方式来表示它们的外形，可以训练的提高精度。
- Branson等[183]提出了一种检测局部的方法，并从多姿态归一化区域（**multiple pose-normalized regions**）提取CNN特征。他们还建立了一个模型，将低层特征层与姿态规范化提取例程相结合，并将更高级别的特征层与**未对齐图像**([image alignment](https://blog.csdn.net/h763247747/article/details/100862863))特征结合起来，以改进分类的准确性。
- Zhang等人【184】提出了一种基于局部的R-CNN方法，该方法可以学习全目标和局部检测器。他们使用 **selective search**（[参考](https://www.cnblogs.com/zyly/p/9259392.html)）来生成局部区域（**part proposal**），并使用非参数几何约束 **non-parametric geometric constraints** (采用了几何约束，将块检测的结果约束在物体检测的一定范围内)来获得更加准确地局部定位。[参考](https://blog.csdn.net/sheng_ai/article/details/41806341)
- Lin等人[186]将局部定位、图像对齐以及图像分类组合到一个识别系统中，其名为 **Deep LAC**。系统由**三个子网组成**：
  - 定位子网用于估计局部位置，输出为框的左上角及右下角点的坐标
  - 对齐子网络接收部件定位结果，执行模板对齐，产生姿态对齐的部件图像 [187],对齐子网络进行平移、缩放、旋转等操作用于姿态对齐区域的生成。同时，该子网络还负责反向传播过程中分类及定位结果的桥接作用。
  - 分类子网络以位姿对齐部分图像作为输入来预测类别标签。
- 他们还提出了一个**阀门连接函数 value linkage function**来连接子网络，并使它们在训练和测试中作为一个整体。

需要注意的是，上述所有方法都是利用部分注释信息进行监督培训的。然而，这些注释并不容易收集，而且这些系统很难扩展和以及难以处理许多类型的细粒度类。为了避免这一问题，一些研究人员开始研究在没有监督的情况下找到局部的部分或区域的问题。

[见微知著——细粒度图像分析进展综述 ](https://www.sohu.com/a/134764420_473283)
- Krause等人[188]将局部学习特征表示的集合用于细粒分类，他们使用**共同分割和对齐** **co-segmentation and alignment** 来生成局部，然后比较各个部分的不同，合并类似的局域。在他们最新的论文中【189】，他们将共分割和对齐结合在一个区分性的混合模型（**discriminative mixture**）中，以生成有利于细粒度分类的部件。
- Zhang等人[190]使用**无监督的选择性搜索生成目标proposal**，然后从多尺度生成的局部proposals中选择有用的部分
- Xiao等人[191]在cnn中应用视觉注意机制（**visual attention**）进行细粒度分类。他们的分类器主要分为是哪个阶段：1. bottom-up attention：获取多个候选框；2. object-level top-down机制选择某个物体的相关的patches；3. 在part level top-down机制中定位区分不同的部分：
![](/img/Two&#32;Level&#32;Attention&#32;Model.png)

   - 将这三个结构结合进行训练得到的网络对于查找前景对象或对象部分，并提取识别特征有很大帮助。
 - 林等人[192]提出了一种用于**细粒度图像分类的双线性模型**。识别结构由两个特征提取器组成。两个特征提取器的输出在图像的每个位置使用外部乘积进行相乘，并汇集以获得 **an image descriptor**

---
## **Object Tracking(对象跟踪)**
对象跟踪的成功很大程度上依赖于目标外观的表示对几个挑战（如视点变化  **view point changes**、光照变化 **illumination changes** 和闭塞**occlusions**）的鲁棒性如何[213] –215]。有许多尝试将CNN应用于对象追踪中。
- Fan等人【216】将CNN作为基础学习器。它学习一个单独的特定于类的网络来跟踪对象。在【216】中，作者利用**位移偏差结构shift-variant architecture**设计了一个CNN tracker/CNN跟踪器。这种架构起着关键的作用，它将CNN模型从检测器转变为跟踪器。这些功能是在离线培训期间学习的。 与仅提取局部空间结构的传统跟踪器不同，此基于CNN的跟踪方法通过考虑两个连续帧的图像来提取空间和时间结构。由于时间信息中的大信号倾向于出现在正在移动的物体附近，因此时间结构为跟踪提供了原始速度信号。
- Li等人【217】提出了一个目标特定的cnn用于目标跟踪，其中cnn在跟踪过程中通过在线获得的新示例进行增量训练。它们使用多个CNNs的候选池作为目标对象的不同实例的数据驱动模型。个体上，每个CNN维护一组特定的kernels，可以利用低级的线索（low-level cues）很好的将物体从他们周围环境中区分出来。这些核在对应的CNN初始化时仅用一个实体训练，之后这些核将会用过在线的方式在每一帧被更新。
  
    与过去对所有的对象训练一个复杂而有效的CNN模型不同，他们在具有实时更新机制的框架内，在CNN中使用一些相对较少的filters。给定一帧，池中最好的CNN将会被选择出来作为判别对象假设。最高分的假设将会被设置为当前物体的检测窗口，被选择的模型使用热启动反向传播重新训练，从而优化结构损失函数。
-  在[218]中，提出了一种cnn目标跟踪方法，以解决目标跟踪问题中手工特征和浅分类器结构的局限性。识别特征首先通过CNN自动学习。为了减轻在模型更新中tracker的**drifting problem**，跟踪器利用初始帧的物体标记信息以及在新获得的观测的图像的 **ground truth**外观信息。使用启发式模式判断是否更新对象外观模型。
- Hong等人[219]提出了一种基于预先训练的cnn的视觉跟踪算法，其中网络最初是为大规模图像分类而训练的，并且学习到的表示转化为描述的物体。在cnn隐藏层的顶部，他们添加了一个在线支持向量机的附加层，用于区别学习目标及其背景。通过反向投影相关的信息到输入图像空间，模型通过SVM的学习来计算特定目标的显著图 **saliency map**。他们利用目标特定的显着性图来获取生成的目标外观模型，并在了解目标空间配置的情况下进行跟踪。

---  
## **Pose Estimation姿态估计**
自从深度结构学习突破了瓶颈，许多利用CNN学习人体姿态估计的多层表达和抽象的研究越来越受重视【220，221】。
- **Deep Pose[222]**是最早将CNN应用于人体姿态估计问题的模型。在这模型中，姿态估计被视为基于CNN的人体关节坐标回归问题。模型提出一个7层瀑布型CNN以整体的方式对人体姿态进行推理。与以前那些通常显式设计图形模型局部检测器的工作不同，DeepPose通过将整个图像作为输入来捕获每个身体关节的完整上下文。

同时，也有一些工作利用CNN学习局部身体部位的表示。
- Ajrun et al. [223]提出了一种基于cnn端到端学习的人体姿态估计方法，其中cnn部分检测器和马尔可夫随机场(MRF)类空间模型被联合训练。使用卷积先验计算图中的**成对potentials(pair-wise potentials)**
- Tompson等人【224使用 **multi-resolution CNN**来计算人体各个部分的热图。与【223】不同，tomposen等人学习身体部分的先验模型并隐式的了解空间模型的结构。具体来说，他们以一对一的方式将身体的每一部分连接到自己和身体的其他部分，然后用一个完全连通的图形来建模空间先验。作为[224]的拓展，Tompson等人[92]提出了一种CNN结构，其中包含了一个经过粗略姿态估计CNN后的位置细化模型。该精化模型是一个孪生网络 （**Siamese network**） [64]，与现成的模型[224]联合训练。
- 类似[224]，Chen等人[225，226]还将图形模型与CNN相结合。他们利用CNN来学习**presense of parts及其他们的空间关系的条件概率**，这些条件概率用于图形模型的**一元和pairwise terms**。学习到的条件概率可以看作是身体姿态的低维表示。
- 还有一种名为**dual-source CNN**[227]的姿态估计方法，它整合了图形模型和整体风格。它以全身形象和局部整体观为输入。 将本地信息和上下文信息结合在一起。

除了CNN的静态图像姿态估计外，最近的研究人员还将CNN应用于视频中的人体姿态估计。
- 基于工作[224]，Jain等人[228]还将RGB特征和运动特性合并到一个 **multi-resolution CNN** 结构中，以进一步提高准确性。cnn以滑动窗口的方式进行姿态估计。CNN的输入是由rgb图像及其相应的运动特征组成的三维张量，输出是包含关节响应图 **response-map** 的三维张量。在每个响应图中，每个位置的值表示存在于该像素位置的对应节点的能量。**multi-response processing** 是通过对输入数据进行下采样并输入其到网络来实现的。

---
## **Test Detection and Recognition 文本检测和识别**
长期以来，图像中文本的识别问题得到了广泛的研究。传统上，**光学字符识别 optical 缠绕出头鸟 recognition(OCR)**是人们关注的焦点。OCR技术主要是在相对受限的视觉环境(例如，干净的背景，良好的文本对齐)中对图像进行文本识别。近年来，随着计算机视觉研究中高级视觉理解的发展趋势，场景图像的文本识别已经成为人们关注的焦点[233，234]。场景图像是在无约束环境下捕获的，在无约束环境中存在大量的外观变化 **appearance variations** ，给现有的OCR技术带来了很大的困难。这种担忧可以通过使用更强大和更丰富的特征表示来缓解，比如CNN模型。在利用CNN提高场景文本识别性能的同时，提出了一些工作。这工作大致可分为三类：
1. 未经识别的文本检测和定位
2. 裁剪文本图像的文本识别
3. 集文本检测和识别为一体的端到端文本识别：

### **Text Detection 文本检测**
将CNN应用于场景文本检测的先驱作品之一是[235]。[235]使用的cnn模型学习裁剪文本图片和非文本场景图片来区分两者。给定输入的多尺度图像金字塔（**the multiscale image pyramid of the input**），然后在CNN filters生成的响应图上检测文本。
- 减少文本检测的搜索空间，Xu等人 [236]提出通过最大稳定极值区域（**Maximally Stable Extremal Regions MSER**）获得一组候选字符，并通过CNN分类过滤候选字符。
- 另一项结合MSER和CNN进行文本检测的工作是[237]。在[237]中，cnn被用来区分类似文本的MSER组件和非文本组件，通过以滑动窗口方式应用CNN，然后再进行**非最大抑制（NMS）**，可以将凌乱的文本组件分开。
- 除了文本的定位之外，还有一项有趣的工作[238]利用CNN来确定输入图像是否包含文本，而不需要知道文本的确切位置。在[238]中，使用MSER获得文本候选，然后将其传递到CNN以生成视觉特征。最后，通过在**词袋（BoW）框架**中聚合CNN特征来构建图像的全局特征。

### **Text Recognition文本识别**
- Goodfellow等人 [239]提出了一个CNN模型，该模型的最后一层具有多个softmax分类器，该模型的制定方式是，每个分类器负责在多像素输入图像的每个序列位置进行字符预测。
- 作为一种不使用**词典和字典 lexicon and dictionary**来识别文本的尝试，Jaderberg等人[240]提出了一种新的条件随机场(CRF)类CNN模型（**a novel Conditional Random Fields (CRF)-like CNN model**），用于场景文本识别中的字符序列预测和 **bigram generation**。

**[自然语言处理中的N-Gram模型详解](https://blog.csdn.net/baimafujinji/article/details/51281816)**

新的文本识别方法以**循环神经网络(RNN)**的变体来补充传统的CNN模型，以更好地建模文本中字符间的序列依赖关系。
- 在[241]中，cnn从通过滑动窗口从获得的字符级图像块中提取丰富的视觉特征，序列标记 （**sequence labelling**）通过**LSTM**【242】提取。
- [243]中提出的方法与[241]非常相似，但在[243]中，使用了词典来提高文本识别性能。

### **End-to-end Text Spotting端到端文本检测**
- 对于端到端的文本检测，Wang等人。[15]应用最初为字符分类训练的CNN模型来执行文本检测。
- 与[15]类似，[244]中提出的cnn模型允许在端到端文本检测系统的四个不同子任务之间进行特征共享：
  1. 文本检测
  2. 字符区分
  3. 字符不敏感分类
  4. bigram分类
- Jaderberg等人[245]以非常全面的方式利用CNN来执行端到端的文本查找。 在[245]中，其提出的系统的主要子任务，即文本边界框过滤，文本边界框回归和文本识别均由单独的CNN模型处理